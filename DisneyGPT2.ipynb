{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eniompw/DisneyGPT/blob/main/DisneyGPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjr8ceOFMqZw",
        "outputId": "6e39b670-2485-41bb-e2b4-3f7a01b65704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 594, done.\u001b[K\n",
            "remote: Counting objects: 100% (329/329), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 594 (delta 295), reused 262 (delta 262), pack-reused 265\u001b[K\n",
            "Receiving objects: 100% (594/594), 763.43 KiB | 20.09 MiB/s, done.\n",
            "Resolving deltas: 100% (352/352), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5aBSOp3QIoM",
        "outputId": "75c8503f-e12d-461b-ff6f-58e0e69fd471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n"
          ]
        }
      ],
      "source": [
        "# install dependencies\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIXj56GPS9e2"
      },
      "outputs": [],
      "source": [
        "# create disney data\n",
        "!cp ./nanoGPT/data/shakespeare/ ./nanoGPT/data/disney/ -r\n",
        "!sed -i \"9s,.*,    data_url = 'https://raw.githubusercontent.com/eniompw/DisneyGPT/main/GrimmsFairyTales.txt',\" ./nanoGPT/data/disney/prepare.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUcJAoK5REPt",
        "outputId": "9cefb1ab-92ed-41ec-cec3-64fcdcd8088f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 128,538 tokens\n",
            "val has 14,168 tokens\n"
          ]
        }
      ],
      "source": [
        "!cd ./nanoGPT/data/disney/ && python prepare.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yke4_cSURw-d",
        "outputId": "1aeb53e4-d360-4219-ee17-fa8e6a19f8f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: out_dir = out\n",
            "Overriding: dataset = disney\n",
            "Overriding: dtype = float16\n",
            "Overriding: eval_iters = 20\n",
            "Overriding: log_interval = 1\n",
            "Overriding: block_size = 64\n",
            "Overriding: batch_size = 12\n",
            "Overriding: n_layer = 4\n",
            "Overriding: n_head = 4\n",
            "Overriding: n_embd = 128\n",
            "Overriding: lr_decay_iters = 2000\n",
            "Overriding: dropout = 0.0\n",
            "Overriding: init_from = gpt2\n",
            "Overriding: eval_interval = 20\n",
            "Overriding: max_iters = 20\n",
            "tokens per iteration will be: 768\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "num decayed parameter tensors: 50, with 123,581,184 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.1958, val loss 4.1440\n",
            "iter 0: loss 4.0662, time 29397.32ms, mfu -100.00%\n",
            "iter 1: loss 4.1738, time 89.38ms, mfu -100.00%\n",
            "iter 2: loss 4.2516, time 90.72ms, mfu -100.00%\n",
            "iter 3: loss 4.1639, time 175.09ms, mfu -100.00%\n",
            "iter 4: loss 3.9823, time 159.54ms, mfu -100.00%\n",
            "iter 5: loss 3.6676, time 159.99ms, mfu 1.15%\n",
            "iter 6: loss 3.5498, time 153.45ms, mfu 1.16%\n",
            "iter 7: loss 3.7009, time 59.04ms, mfu 1.35%\n",
            "iter 8: loss 3.5610, time 157.72ms, mfu 1.34%\n",
            "iter 9: loss 3.6088, time 160.55ms, mfu 1.32%\n",
            "iter 10: loss 3.4369, time 160.53ms, mfu 1.30%\n",
            "iter 11: loss 3.3595, time 163.48ms, mfu 1.28%\n",
            "iter 12: loss 3.4678, time 159.56ms, mfu 1.27%\n",
            "iter 13: loss 3.2645, time 160.41ms, mfu 1.26%\n",
            "iter 14: loss 3.1863, time 162.77ms, mfu 1.25%\n",
            "iter 15: loss 3.5046, time 160.56ms, mfu 1.24%\n",
            "iter 16: loss 3.2225, time 160.45ms, mfu 1.23%\n",
            "iter 17: loss 3.2995, time 162.13ms, mfu 1.22%\n",
            "iter 18: loss 3.0156, time 161.49ms, mfu 1.21%\n",
            "iter 19: loss 3.1961, time 161.53ms, mfu 1.20%\n",
            "step 20: train loss 3.2613, val loss 3.2643\n",
            "saving checkpoint to out\n",
            "iter 20: loss 3.2156, time 6764.62ms, mfu 1.09%\n"
          ]
        }
      ],
      "source": [
        "!cd ./nanoGPT && python train.py config/train_shakespeare_char.py --out_dir='out' --dataset=disney --dtype=float16 --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --lr_decay_iters=2000 --dropout=0.0 --init_from='gpt2' --eval_interval=20 --max_iters=20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1fJeRAdRl5e",
        "outputId": "07c444a1-bf20-4716-c8a4-b6d6c1512b2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: dtype = float16\n",
            "Overriding: num_samples = 5\n",
            "Overriding: max_new_tokens = 100\n",
            "Overriding: start =  \n",
            "number of parameters: 123.65M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            " �Now you have\n",
            "lived, have we?’\n",
            "\n",
            "And that evening, the father,\n",
            "agrees that he is now under the bed, and that the\n",
            "mother has found him.’ The next morning the\n",
            "mother and the servant followed their\n",
            "back, but the eldest of them said: ‘There is nothing\n",
            "left to do but lie still;’\n",
            " Then the third servant went to the door through the corner and\n",
            "cut a hole on the right flank\n",
            "---------------\n",
            " �\n",
            "We shall kill thee\n",
            "of all the meat\n",
            "that we have.’ At last the\n",
            "cow flew into the sky, and\n",
            "had a fair head and herring-house, and then the\n",
            "methpotted bird rushed across the garden\n",
            "and ate the eggs. The maid stood watch in one room with a\n",
            "cup of ale, and said: ‘I should like to see a golden ale,\n",
            "but my dear wife will give you no time to do\n",
            "---------------\n",
            " �s mother's side, and said: ‘Oh! what a monster is this’s?’ ‘I tell you,’ said the son, ‘and that was the last goblet, and the time did not come when I thought\n",
            "of it.’ The time very nearly came, and came, moreover, when the king\n",
            "of Sweden showed up, and said to the queen: ‘And what\n",
            "good\n",
            "do you think of me\n",
            "---------------\n",
            " �s;’s she said. ‘I wish to hear the birds\n",
            "of*the forest,’ said she. ‘I have found many trees here.’\n",
            "\n",
            "The king answered. ‘Well,’ said the queen, ‘There are many!\n",
            "\n",
            "\n",
            "\n",
            "‘Truly, then, tell me the number.’\n",
            "\n",
            "The king said: ‘No,’ said the queen, ‘I will tell\n",
            "---------------\n",
            " ��\n",
            "\n",
            "And then in reply said: ‘Well,\n",
            "what do you have?’ ‘Not very well’; but it would be\n",
            "for one to think that there was no good way, and did not know what\n",
            "the best way was, so it was said to her. ‘Well,\n",
            "there is one which will be for you.’ Then she\n",
            "went into the garden, and found a whole fair\n",
            "house, devoted to the singing\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "!cd ./nanoGPT && python sample.py --dtype=float16 --num_samples=5 --max_new_tokens=100 --start=\" \""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMWFxcZ+eKeH7CSRvbChcYQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}